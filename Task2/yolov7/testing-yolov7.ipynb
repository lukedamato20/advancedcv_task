{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f407d05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lldam\\OneDrive\\UOM\\3rd Year\\Advanced Computer Vision\\advancedcv_task\\Task2\\yolov7\\yolov7_custom.pt\n"
     ]
    }
   ],
   "source": [
    "# import keras\n",
    "import keras\n",
    "\n",
    "# import keras_retinanet\n",
    "from utils.plots import plot_one_box\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import miscellaneous modules\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# set tf backend to allow memory to grow, instead of claiming everything\n",
    "import tensorflow as tf\n",
    "\n",
    "path = os.getcwd()\n",
    " \n",
    "model_path = os.path.join(path, \"yolov7_custom.pt\")\n",
    "image_path = os.path.join(path, \"/data/testing\", \"imgTEST.jpg\")\n",
    "image_output_path = os.path.join(path, \"/runs/detect\", \"imgDETECT.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45058b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "image = read_image_bgr(image_path)\n",
    "\n",
    "# copy to draw on\n",
    "draw = image.copy()\n",
    "draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# preprocess image for network\n",
    "image = preprocess_image(image)\n",
    "image, scale = resize_image(image)\n",
    "\n",
    "# process image\n",
    "start = time.time()\n",
    "boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis=0))\n",
    "print(\"processing time: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f80c991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['yolov7_custom.pt'], source='data/testing/imgTEST.jpg', img_size=640, conf_thres=0.6, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project='runs/detect', name='exp', exist_ok=False, no_trace=False)\n",
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "Bounding boxes coordinates are:  [0.39125001430511475, 0.4373913109302521, 0.09749999642372131, 0.05043478310108185]\n",
      "1 crane, Done. (405.3ms) Inference, (5.5ms) NMS\n",
      " The image with the result is saved in: runs\\detect\\exp4\\imgTEST.jpg\n",
      "Done. (0.448s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  2023-1-1 torch 1.13.1+cpu CPU\n",
      "\n",
      "Model Summary: 314 layers, 36481772 parameters, 6194944 gradients\n",
      "C:\\Users\\lldam\\anaconda3\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights yolov7_custom.pt --conf 0.60 --img-size 640 --source \"data/testing/imgTEST.jpg\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
